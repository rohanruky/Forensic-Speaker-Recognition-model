{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e947157c-f0be-4449-ac18-d6fd70bb0312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import assemblyai as aai\n",
    "from langdetect import detect  # Language detection library\n",
    "from collections import Counter\n",
    "import librosa  # For audio processing and feature extraction\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import gradio as gr\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Download NLTK stop words if not already available\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set your AssemblyAI API key\n",
    "aai.settings.api_key = \"285022177ce545adb643b3711f5d062d\"\n",
    "\n",
    "# Predefined list of Hindi stop words\n",
    "hindi_stopwords = set([\n",
    "    'और', 'भी', 'का', 'के', 'कि', 'को', 'में', 'से', 'है', 'यह', 'था', 'तक', 'तो', 'नहीं', 'पर', 'ही', 'जैसे', 'क्या', 'किस', 'कौन', 'वे', 'हम', 'आप', 'जो'\n",
    "])\n",
    "\n",
    "def noise_reduction(audio, sr):\n",
    "    \"\"\" Apply noise reduction to the audio signal \"\"\"\n",
    "    # Compute the short-time Fourier transform\n",
    "    stft = librosa.stft(audio)\n",
    "    spectrogram = np.abs(stft)\n",
    "\n",
    "    # Estimate the noise profile using median filtering\n",
    "    noise_profile = np.median(spectrogram, axis=1, keepdims=True)\n",
    "    \n",
    "    # Subtract the noise profile\n",
    "    spectrogram_denoised = np.maximum(spectrogram - noise_profile, 0)\n",
    "    \n",
    "    # Inverse STFT to reconstruct the denoised audio\n",
    "    stft_denoised = spectrogram_denoised * np.exp(1j * np.angle(stft))\n",
    "    denoised_audio = librosa.istft(stft_denoised)\n",
    "    return denoised_audio\n",
    "\n",
    "def bandpass_filter(audio, sr, lowcut=300, highcut=3000):\n",
    "    \"\"\" Apply a bandpass filter to isolate speech frequencies \"\"\"\n",
    "    nyquist = 0.5 * sr\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(1, [low, high], btype='band')\n",
    "    filtered_audio = lfilter(b, a, audio)\n",
    "    return filtered_audio\n",
    "\n",
    "def preprocess_audio(audio_path):\n",
    "    \"\"\" Load, apply noise reduction, and preprocess audio \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=None)\n",
    "        # Apply noise reduction\n",
    "        audio_denoised = noise_reduction(audio, sr)\n",
    "        # Apply bandpass filter\n",
    "        audio_filtered = bandpass_filter(audio_denoised, sr)\n",
    "        return audio_filtered, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error in audio preprocessing: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\" Transcribe audio and return transcript text and words with timestamps \"\"\"\n",
    "    config = aai.TranscriptionConfig(\n",
    "        language_detection=True,\n",
    "        language_confidence_threshold=0.4\n",
    "    )\n",
    "    transcriber = aai.Transcriber(config=config)\n",
    "    try:\n",
    "        transcript = transcriber.transcribe(audio_path)\n",
    "        if transcript and transcript.words:\n",
    "            words = [{'text': word.text, 'start_time': word.start, 'end_time': word.end} for word in transcript.words]\n",
    "            return transcript.text, words\n",
    "        else:\n",
    "            return None, []\n",
    "    except Exception as e:\n",
    "        return None, []\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\" Detect language of the given text \"\"\"\n",
    "    return detect(text)\n",
    "\n",
    "def remove_stopwords(words, language):\n",
    "    \"\"\" Remove stop words from the list of words based on language \"\"\"\n",
    "    if language == 'en':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    elif language == 'hi':\n",
    "        stop_words = hindi_stopwords\n",
    "    else:\n",
    "        return words  # If language is not supported, return unfiltered words\n",
    "    filtered_words = [word for word in words if word['text'].lower() not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def get_common_words(words1, words2):\n",
    "    \"\"\" Find common words between two lists of words \"\"\"\n",
    "    words1_texts = [word['text'] for word in words1]\n",
    "    words2_texts = [word['text'] for word in words2]\n",
    "    counter1 = Counter(words1_texts)\n",
    "    counter2 = Counter(words2_texts)\n",
    "    common_words_texts = list((counter1 & counter2).elements())\n",
    "    common_words_1 = [word for word in words1 if word['text'] in common_words_texts]\n",
    "    common_words_2 = [word for word in words2 if word['text'] in common_words_texts]\n",
    "    return common_words_1, common_words_2\n",
    "\n",
    "def extract_audio_features(audio_path, word_start_time, word_end_time):\n",
    "    \"\"\" Extract audio features for a specific word in the audio \"\"\"\n",
    "    y, sr = preprocess_audio(audio_path)\n",
    "    if y is None:\n",
    "        return {}\n",
    "    offset = word_start_time / 1000\n",
    "    duration = (word_end_time - word_start_time) / 1000\n",
    "    y_segment = y[int(offset * sr):int((offset + duration) * sr)]\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=y_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y_segment, sr=sr).mean()\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y_segment, sr=sr).mean()\n",
    "    rms = librosa.feature.rms(y=y_segment).mean()\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=y_segment, sr=sr)\n",
    "    pitch = pitches[pitches > 0].mean() if len(pitches[pitches > 0]) > 0 else 0\n",
    "    return {\n",
    "        'mfcc': mfcc,\n",
    "        'spectral_centroid': spectral_centroid,\n",
    "        'spectral_bandwidth': spectral_bandwidth,\n",
    "        'rms': rms,\n",
    "        'zero_crossing_rate': zero_crossing_rate,\n",
    "        'pitch': pitch\n",
    "    }\n",
    "\n",
    "def compare_features(features1, features2):\n",
    "    \"\"\" Compare features between two samples and provide detailed analysis \"\"\"\n",
    "    analysis = []\n",
    "    mfcc_diff = np.linalg.norm(np.array(features1['mfcc']) - np.array(features2['mfcc']))\n",
    "    analysis.append(f\"MFCC Difference: {mfcc_diff:.2f}\")\n",
    "    spectral_centroid_diff = abs(features1['spectral_centroid'] - features2['spectral_centroid'])\n",
    "    analysis.append(f\"Spectral Centroid Difference: {spectral_centroid_diff:.2f} Hz\")\n",
    "    spectral_bandwidth_diff = abs(features1['spectral_bandwidth'] - features2['spectral_bandwidth'])\n",
    "    analysis.append(f\"Spectral Bandwidth Difference: {spectral_bandwidth_diff:.2f} Hz\")\n",
    "    rms_diff = abs(features1['rms'] - features2['rms'])\n",
    "    analysis.append(f\"RMS Energy Difference: {rms_diff:.2f}\")\n",
    "    zcr_diff = abs(features1['zero_crossing_rate'] - features2['zero_crossing_rate'])\n",
    "    analysis.append(f\"Zero Crossing Rate Difference: {zcr_diff:.4f}\")\n",
    "    pitch_diff = abs(features1['pitch'] - features2['pitch'])\n",
    "    analysis.append(f\"Pitch Difference: {pitch_diff:.2f} Hz\")\n",
    "    total_diff = mfcc_diff + spectral_centroid_diff + spectral_bandwidth_diff + rms_diff + zcr_diff + pitch_diff\n",
    "    analysis.append(f\"Total Feature Difference for this Word: {total_diff:.2f}\")\n",
    "    return analysis, total_diff\n",
    "\n",
    "def generate_pdf(report, pdf_path=\"Analysis_Report.pdf\"):\n",
    "    \"\"\" Generate a PDF report from the analysis text \"\"\"\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, report)\n",
    "    pdf.output(pdf_path)\n",
    "    return pdf_path\n",
    "\n",
    "def process_audio(audio_path_1, audio_path_2):\n",
    "    try:\n",
    "        # Transcribe audio\n",
    "        transcript_text_1, words_1 = transcribe_audio(audio_path_1)\n",
    "        transcript_text_2, words_2 = transcribe_audio(audio_path_2)\n",
    "\n",
    "        if not transcript_text_1 or not transcript_text_2:\n",
    "            return \"Error: One or both audio files could not be transcribed.\", None\n",
    "\n",
    "        # Detect language\n",
    "        language_1 = detect_language(transcript_text_1)\n",
    "        language_2 = detect_language(transcript_text_2)\n",
    "\n",
    "        if language_1 != language_2:\n",
    "            return f\"Error: Different languages detected (Sample 1: {language_1}, Sample 2: {language_2}).\", None\n",
    "\n",
    "        # Remove stopwords and find common words\n",
    "        words_1_filtered = remove_stopwords(words_1, language_1)\n",
    "        words_2_filtered = remove_stopwords(words_2, language_2)\n",
    "        common_words_1, common_words_2 = get_common_words(words_1_filtered, words_2_filtered)\n",
    "\n",
    "        if not common_words_1 or not common_words_2:\n",
    "            return \"No common words found after removing stop words.\", None\n",
    "\n",
    "        # Compare features for common words\n",
    "        analysis_report = []\n",
    "        total_diff_sum = 0\n",
    "        word_count = len(common_words_1)\n",
    "\n",
    "        for i, (word_1, word_2) in enumerate(zip(common_words_1, common_words_2)):\n",
    "            features_1 = extract_audio_features(audio_path_1, word_1['start_time'], word_1['end_time'])\n",
    "            features_2 = extract_audio_features(audio_path_2, word_2['start_time'], word_2['end_time'])\n",
    "            analysis, total_diff = compare_features(features_1, features_2)\n",
    "            analysis_report.append(f\"\\n--- Word {i+1}: {word_1['text']} ---\")\n",
    "            analysis_report.extend(analysis)\n",
    "            total_diff_sum += total_diff\n",
    "\n",
    "        average_diff = total_diff_sum / word_count if word_count > 0 else 0\n",
    "        analysis_report.append(f\"\\nAverage Feature Difference: {average_diff:.2f}\")\n",
    "\n",
    "        # Conclusion\n",
    "        if average_diff < 1300.0:\n",
    "            analysis_report.append(\"\\nConclusion: The speakers are likely the same.\")\n",
    "        else:\n",
    "            analysis_report.append(\"\\nConclusion: The speakers are likely different.\")\n",
    "\n",
    "        final_report = \"\\n\".join(analysis_report)\n",
    "        pdf_path = generate_pdf(final_report)\n",
    "        return final_report, pdf_path\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", None\n",
    "\n",
    "# Gradio Interface\n",
    "def interface(audio1, audio2):\n",
    "    report, pdf_path = process_audio(audio1, audio2)\n",
    "    return report, pdf_path\n",
    "\n",
    "inputs = [\n",
    "    gr.Audio(label=\"Upload Audio File 1\", type=\"filepath\"),\n",
    "    gr.Audio(label=\"Upload Audio File 2\", type=\"filepath\")\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    gr.Textbox(label=\"Analysis Report\"),\n",
    "    gr.File(label=\"Download PDF Report\")\n",
    "]\n",
    "\n",
    "gr.Interface(\n",
    "    fn=interface,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    title=\"Forensic Speaker Recognition\",\n",
    "    description=\"Upload two audio samples to check if they are spoken by the same individual. Download the report as a PDF.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75697d9-17dc-4dae-ae17-056535c1cc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
